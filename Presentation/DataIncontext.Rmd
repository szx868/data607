---
title: "Untitled"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Rselenium 

Many websites are difficult to scrap because they use JavaScript and jQuery to dynamically extract data from the database. For example, on common social media sites such as LinkedIn or Facebook, when you scroll down the page, new content will be loaded and the URL will not change. These sites are difficult to scrap, To simpify scraping task, we can adjust the URL based on a certain system pattern to load a new page.

For example, if we check the Grainger website, we will see that the URL changes systematically, for example.
https://www.grainger.com/
```{r}
my_data <- read.csv("https://raw.githubusercontent.com/szx868/data607/master/Presentation/infile.txt",header=T)
my_data
```


```{r cars}
library(RSelenium)
library(tidyverse)


cprof <- list(chromeOptions = 
                list(extensions = 
                       list(base64enc::base64encode("VPN_PROXY_MASTER.crx"))
                ))


rD <- rsDriver(port = 4449L,extraCapabilities=cprof, browser ="chrome",chromever = "latest")
remDr <- rD[["client"]]
remDr$setTimeout(type = 'page load', milliseconds = 120000)
remDr$setTimeout(type = 'implicit', milliseconds = 120000)
remDr$navigate("chrome-extension://lnfdmdhmfbimhhpaeocncdlhiodoblbd/popup/popup.html")
time <- 5
Sys.sleep(time)
webElem <- remDr$findElement("css", "body")
# find button
morereviews <- remDr$findElement(using = 'css selector', ".start-btn")
# click button
morereviews$clickElement()
# wait
Sys.sleep(8)
remDr$setTimeout(type = 'page load', milliseconds = 120000)
remDr$setTimeout(type = 'implicit', milliseconds = 120000)
remDr$navigate("https://www.grainger.com/")
webElem2 <- remDr$findElement("css", "body")
morereviews2 <- remDr$findElement(using = "name", value = "searchQuery")$sendKeysToElement(list('3d264',key="enter"))
Sys.sleep(5) # give the page time to fully load
html <- remDr$getPageSource()[[1]]

```

```{r}
remDr$close()
rD$server$stop()
rm(rD, remDr)
gc()
system("taskkill /im java.exe /f", intern=FALSE, ignore.stdout=FALSE)
```

![image info](capture3.png)
![image info](capture.png)
![image info](capture2.png)
```{r}
library(rvest)
library("knitr")

item.desc <- read_html(html) %>% # parse HTML
  html_nodes(".specifications__description") %>% # extract class nodes with class = "specifications__description"
  html_text() 
item.value <- read_html(html) %>% # parse HTML
  html_nodes(".specifications__value") %>% # extract class nodes with class = "specifications__item"
  html_text() 
tech.spec <- cbind(item.desc, item.value)
tech.spec


```

```{r}
count <- 0 
for(i in 1:nrow(my_data)){
  count <- count + 1
  print(count)
  print(my_data[i,])
}
```

```{r echo=False}
library(RSelenium)
library(tidyverse)
library(rvest)
library("knitr")
df = data.frame(x = character(), y = character(), z = character())

      
for (i in 1:3){
      cprof <- list(chromeOptions = 
                list(extensions = 
                       list(base64enc::base64encode("VPN_PROXY_MASTER.crx"))
                ))
      rD <- rsDriver(port = 4449L,extraCapabilities=cprof, browser ="chrome",chromever = "latest")
      remDr <- rD[["client"]]
      remDr$setTimeout(type = 'page load', milliseconds = 120000)
      remDr$setTimeout(type = 'implicit', milliseconds = 120000)
      remDr$navigate("chrome-extension://lnfdmdhmfbimhhpaeocncdlhiodoblbd/popup/popup.html")
      Sys.sleep(5)
      webElem <- remDr$findElement("css", "body")
      # find button
      morereviews <- remDr$findElement(using = 'css selector', ".start-btn")
      # click button
      morereviews$clickElement()
      # wait
      Sys.sleep(8)
      remDr$setTimeout(type = 'page load', milliseconds = 120000)
      remDr$setTimeout(type = 'implicit', milliseconds = 120000)
      remDr$navigate("https://www.grainger.com/")
      webElem2 <- remDr$findElement("css", "body")
      morereviews2 <- remDr$findElement(using = "name", value = "searchQuery")$sendKeysToElement(list(my_data[i,],key="enter"))
      Sys.sleep(5) # give the page time to fully load
      html <- remDr$getPageSource()[[1]]
      remDr$close()
      rD$server$stop()
      rm(rD, remDr)
      gc()
      system("taskkill /im java.exe /f", intern=FALSE, ignore.stdout=FALSE)
      item.desc <- read_html(html) %>% # parse HTML
        html_nodes(".specifications__description") %>% # extract class nodes with class = "specifications__description"
        html_text() 
      item.value <- read_html(html) %>% # parse HTML
        html_nodes(".specifications__value") %>% # extract class nodes with class = "specifications__item"
      html_text() 
      df <- rbind(df, data.frame(x = item.desc, y = item.value, z = my_data[i,]))
    
}
```

```{r}
df
```

## Inconclusion

RSelenium provides many other functions, which are not described here. This is an introduction to how Selenium works and how to interact with other common R packages such as rvest. Among other features of Selenium, you can take screenshots, click on specific links or sections on the page, scroll down the page and enter any keyboard strokes into any part of the web page. When combined with classic crawling technology, it has a wide range of uses and can crawl almost any website.


